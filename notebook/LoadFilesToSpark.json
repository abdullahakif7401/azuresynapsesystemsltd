{
	"name": "LoadFilesToSpark",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "apachesparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "68010af6-672d-47c0-bb7d-173a42490f06"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/9200c36b-c7aa-4fd2-9365-8bff84d447a1/resourceGroups/Azure_Synapse_Analytics_Workshop_Ltd/providers/Microsoft.Synapse/workspaces/azuresynapseanalyticsworkshopltd/bigDataPools/apachesparkpool",
				"name": "apachesparkpool",
				"type": "Spark",
				"endpoint": "https://azuresynapseanalyticsworkshopltd.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apachesparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Replace these variables with your own values\r\n",
					"account_name = \"azuresynapseanalyticstem\"\r\n",
					"container_name = \"synapse\"\r\n",
					"directory_path = \"Input\"\r\n",
					"storage_account_key = \"SXYxW+DFDjNmisynrebHRkTXDcX4kuZIFb8Z/JmS4X+WBPLKRTijFAYjBiS9WsVrpbIiT/KphL0P+ASt3tZ6PQ==\""
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Initialize Spark session\r\n",
					"spark = SparkSession.builder \\\r\n",
					"    .appName(\"List CSV Files in ADLS Gen2 Container\") \\\r\n",
					"    .config(\"fs.azure.account.key.\"+account_name+\".dfs.core.windows.net\", storage_account_key) \\\r\n",
					"    .getOrCreate()"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define the schema of the SQL database\r\n",
					"sql_db_schema = StructType([\r\n",
					"    StructField(\"school\", StringType(), True),\r\n",
					"    StructField(\"sex\", StringType(), True),\r\n",
					"    StructField(\"age\", IntegerType(), True),\r\n",
					"    StructField(\"address\", StringType(), True),\r\n",
					"    StructField(\"famsize\", StringType(), True),\r\n",
					"    StructField(\"Pstatus\", StringType(), True),\r\n",
					"    StructField(\"Medu\", IntegerType(), True),\r\n",
					"    StructField(\"Fedu\", IntegerType(), True),\r\n",
					"    StructField(\"Mjob\", StringType(), True),\r\n",
					"    StructField(\"Fjob\", StringType(), True),\r\n",
					"    StructField(\"reason\", StringType(), True),\r\n",
					"    StructField(\"guardian\", StringType(), True),\r\n",
					"    StructField(\"traveltime\", IntegerType(), True),\r\n",
					"    StructField(\"studytime\", IntegerType(), True),\r\n",
					"    StructField(\"failures\", IntegerType(), True),\r\n",
					"    StructField(\"schoolsup\", StringType(), True),\r\n",
					"    StructField(\"famsup\", StringType(), True),\r\n",
					"    StructField(\"paid\", StringType(), True),\r\n",
					"    StructField(\"activities\", StringType(), True),\r\n",
					"    StructField(\"nursery\", StringType(), True),\r\n",
					"    StructField(\"higher\", StringType(), True),\r\n",
					"    StructField(\"internet\", StringType(), True),\r\n",
					"    StructField(\"romantic\", StringType(), True),\r\n",
					"    StructField(\"famrel\", IntegerType(), True),\r\n",
					"    StructField(\"freetime\", IntegerType(), True),\r\n",
					"    StructField(\"goout\", IntegerType(), True),\r\n",
					"    StructField(\"Dalc\", IntegerType(), True),\r\n",
					"    StructField(\"Walc\", IntegerType(), True),\r\n",
					"    StructField(\"health\", IntegerType(), True),\r\n",
					"    StructField(\"absences\", IntegerType(), True),\r\n",
					"    StructField(\"G1\", IntegerType(), True),\r\n",
					"    StructField(\"G2\", IntegerType(), True),\r\n",
					"    StructField(\"G3\", IntegerType(), True)\r\n",
					"])"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define the ADLS Gen2 path\r\n",
					"adls_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (container_name, account_name, directory_path)\r\n",
					"# List CSV files in the specified directory\r\n",
					"csv_files = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()).listStatus(spark._jvm.org.apache.hadoop.fs.Path(adls_path))\r\n",
					"csv_file_abfss = []\r\n",
					"# Display the list of CSV files\r\n",
					"for file_status in csv_files:\r\n",
					"    file_path = str(file_status.getPath())\r\n",
					"    if file_path.endswith(\".csv\"):\r\n",
					"        csv_file_abfss.append(file_path)\r\n",
					"\r\n",
					"for csv_file_path in csv_file_abfss:\r\n",
					"    # Read the CSV file with the specified schema\r\n",
					"    df = spark.read.options(inferschema=True).csv(csv_file_path, header=True)\r\n",
					"\r\n",
					"    # Check if the DataFrame schema matches the specified schema\r\n",
					"    if df.schema == sql_db_schema:\r\n",
					"        success_adls_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (container_name, account_name, \"Success\")\r\n",
					"        df.write.csv(success_adls_path, header=True, mode=\"overwrite\")\r\n",
					"        df.write.saveAsTable('student', format=\"parquet\", mode=\"overwrite\")\r\n",
					"        print(f\"{csv_file_path} is consistent with the schema.\")\r\n",
					"    else:\r\n",
					"        success_adls_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (container_name, account_name, \"Failure\")\r\n",
					"        df.write.csv(success_adls_path, header=True, mode=\"overwrite\")\r\n",
					"        print(f\"{csv_file_path} does not match the specified schema.\")"
				],
				"execution_count": 11
			}
		]
	}
}
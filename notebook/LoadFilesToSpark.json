{
	"name": "LoadFilesToSpark",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "apache",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "d65082b4-e8ed-47df-91d0-10718c3802ee"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/9200c36b-c7aa-4fd2-9365-8bff84d447a1/resourceGroups/Azure_Synapse_Analytics_Workshop_Ltd/providers/Microsoft.Synapse/workspaces/azuresynapseanalyticsworkshopltd/bigDataPools/apache",
				"name": "apache",
				"type": "Spark",
				"endpoint": "https://azuresynapseanalyticsworkshopltd.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apache",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### **Imports**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, IntegerType\r\n",
					"import configparser"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### **Reading from Configuration File**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"config = configparser.ConfigParser()\r\n",
					"config.read(r'abfss://synapse@azuresynapseanalyticstem.dfs.core.windows.net/Configurations/config.ini')\r\n",
					"input_location = config.get('paths', 'adls_path')\r\n",
					"success_location = config.get('paths', 'success_adls_path')\r\n",
					"failure_location = config.get('paths', 'failure_adls_path')\r\n",
					"sql_db_schema_fromconfig = config.get('schema', 'sql_db_schema')"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# account_name = \"azuresynapseanalyticstem\"\r\n",
					"# container_name = \"synapse\"\r\n",
					"# directory_path = \"Input\"\r\n",
					"# storage_account_key = \"SXYxW+DFDjNmisynrebHRkTXDcX4kuZIFb8Z/JmS4X+WBPLKRTijFAYjBiS9WsVrpbIiT/KphL0P+ASt3tZ6PQ==\""
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### **Initializing Spark Session**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark = SparkSession.builder \\\r\n",
					"    .appName(\"List CSV Files in ADLS Gen2 Container\") \\\r\n",
					"    .config(\"fs.azure.account.key.\"+account_name+\".dfs.core.windows.net\", storage_account_key) \\\r\n",
					"    .getOrCreate()"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### **Defining the schema of the SQL Database**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def read_schema(sql_db_schema):\r\n",
					"    dtypes = {\r\n",
					"        \"StringType()\":StringType(),\r\n",
					"        \"IntegerType()\":IntegerType()\r\n",
					"    }\r\n",
					"\r\n",
					"    split_values = sql_db_schema.split(\",\")\r\n",
					"    db_schema = StructType()\r\n",
					"    for i in split_values:\r\n",
					"        x = i.split(\" \")\r\n",
					"        sch.add(x[0], dtypes[x[1]], True)\r\n",
					"    return db_schema"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"sql_db_schema = StructType([\r\n",
					"    StructField(\"school\", StringType(), True),\r\n",
					"    StructField(\"sex\", StringType(), True),\r\n",
					"    StructField(\"age\", IntegerType(), True),\r\n",
					"    StructField(\"address\", StringType(), True),\r\n",
					"    StructField(\"famsize\", StringType(), True),\r\n",
					"    StructField(\"Pstatus\", StringType(), True),\r\n",
					"    StructField(\"Medu\", IntegerType(), True),\r\n",
					"    StructField(\"Fedu\", IntegerType(), True),\r\n",
					"    StructField(\"Mjob\", StringType(), True),\r\n",
					"    StructField(\"Fjob\", StringType(), True),\r\n",
					"    StructField(\"reason\", StringType(), True),\r\n",
					"    StructField(\"guardian\", StringType(), True),\r\n",
					"    StructField(\"traveltime\", IntegerType(), True),\r\n",
					"    StructField(\"studytime\", IntegerType(), True),\r\n",
					"    StructField(\"failures\", IntegerType(), True),\r\n",
					"    StructField(\"schoolsup\", StringType(), True),\r\n",
					"    StructField(\"famsup\", StringType(), True),\r\n",
					"    StructField(\"paid\", StringType(), True),\r\n",
					"    StructField(\"activities\", StringType(), True),\r\n",
					"    StructField(\"nursery\", StringType(), True),\r\n",
					"    StructField(\"higher\", StringType(), True),\r\n",
					"    StructField(\"internet\", StringType(), True),\r\n",
					"    StructField(\"romantic\", StringType(), True),\r\n",
					"    StructField(\"famrel\", IntegerType(), True),\r\n",
					"    StructField(\"freetime\", IntegerType(), True),\r\n",
					"    StructField(\"goout\", IntegerType(), True),\r\n",
					"    StructField(\"Dalc\", IntegerType(), True),\r\n",
					"    StructField(\"Walc\", IntegerType(), True),\r\n",
					"    StructField(\"health\", IntegerType(), True),\r\n",
					"    StructField(\"absences\", IntegerType(), True),\r\n",
					"    StructField(\"G1\", IntegerType(), True),\r\n",
					"    StructField(\"G2\", IntegerType(), True),\r\n",
					"    StructField(\"G3\", IntegerType(), True)\r\n",
					"])"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### **Data Reading and Processing**\r\n",
					"Defining the ADLS Gen2 path and using it to list CSV files in the specified directory. Moving on, adding the CSV files path to an array. Further, reading those files independently and checking if the schema is coherent with the specified schema; then the file is moved to a success folder otherwise it is moved to a failure folder. The file/files moved to the success folder are also loaded into a database table."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define the ADLS Gen2 path\r\n",
					"adls_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (container_name, account_name, directory_path)\r\n",
					"# List CSV files in the specified directory\r\n",
					"csv_files = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()).listStatus(spark._jvm.org.apache.hadoop.fs.Path(adls_path))\r\n",
					"csv_file_abfss = []\r\n",
					"# Adding the CSV files to an Array\r\n",
					"for file_status in csv_files:\r\n",
					"    file_path = str(file_status.getPath())\r\n",
					"    if file_path.endswith(\".csv\"):\r\n",
					"        csv_file_abfss.append(file_path)\r\n",
					"\r\n",
					"for csv_file_path in csv_file_abfss:\r\n",
					"    # Read the CSV file with the specified schema\r\n",
					"    df = spark.read.options(inferschema=True).csv(csv_file_path, header=True)\r\n",
					"\r\n",
					"    # Check if the DataFrame schema matches the specified schema\r\n",
					"    if df.schema == sql_db_schema:\r\n",
					"        success_adls_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (container_name, account_name, \"Success\")\r\n",
					"        df.write.csv(success_adls_path, header=True, mode=\"overwrite\")\r\n",
					"        df.write.saveAsTable('stg_ student', format=\"parquet\", mode=\"overwrite\")\r\n",
					"        print(f\"{csv_file_path} is consistent with the schema.\")\r\n",
					"    else:\r\n",
					"        failure_adls_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (container_name, account_name, \"Failure\")\r\n",
					"        df.write.csv(failure_adls_path, header=True, mode=\"overwrite\")\r\n",
					"        print(f\"{csv_file_path} does not match the specified schema.\")"
				],
				"execution_count": 5
			}
		]
	}
}